{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map parser\n",
    "Use mapparse.py to fnamed unique tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import csv, sqlite3\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"tampa_florida.osm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapparser.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile mapparser.py\n",
    "\n",
    "def count_tags(filename):\n",
    "    tag_count = {}\n",
    "    for _, element in ET.iterparse(filename, events=(\"start\",)):\n",
    "        add_tag(element.tag, tag_count)\n",
    "    return tag_count\n",
    "\n",
    "def add_tag(tag, tag_count):\n",
    "    if tag in tag_count:\n",
    "        tag_count[tag] += 1\n",
    "    else:\n",
    "        tag_count[tag] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 31857,\n",
      " 'nd': 1957582,\n",
      " 'node': 1655566,\n",
      " 'osm': 1,\n",
      " 'relation': 1252,\n",
      " 'tag': 1131585,\n",
      " 'way': 182866}\n"
     ]
    }
   ],
   "source": [
    "tags = count_tags(OSMFILE)\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Use tags.py to fnamed patterns in the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%writefile tags.py\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        key = element.get(\"k\")\n",
    "        #print key\n",
    "        if problemchars.search(key):\n",
    "            keys['problemchars'] += 1\n",
    "            #print '--> problemchars'\n",
    "        elif lower_colon.search(key):\n",
    "            keys['lower_colon'] += 1\n",
    "            #print '--> lower_colon'\n",
    "        elif lower.search(key):\n",
    "            keys['lower'] += 1\n",
    "            #print '--> lower'\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "            #print '--> other'\n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 575997, 'lower_colon': 520908, 'other': 34675, 'problemchars': 5}\n"
     ]
    }
   ],
   "source": [
    "keys = process_map(OSMFILE)\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users\n",
    "fnamed user ids by using users.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%writefile users.py\n",
    "\n",
    "def get_user(element):\n",
    "    return\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if 'uid' in element.attrib:\n",
    "            users.add(element.get('uid'))\n",
    "\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = process_map(OSMFILE)\n",
    "pprint.pprint(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit\n",
    "Audit and clean data with audit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting audit.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile audit.py\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "direction_re = re.compile(r'\\w*(North|South|East|West|Northeast|Northwest|Southeast|Southwest|S|NE|W|N|E|SE|N.)$')\n",
    "\n",
    "direction_mapping = {\"N\":\"North\",\n",
    "                    \"S\":\"South\",\n",
    "                    \"NE\":\"Northeast\",\n",
    "                    \"W\":\"West\",\n",
    "                    \"E\":\"East\",\n",
    "                    \"SE\": \"Southeast\"}\n",
    "\n",
    "expected = [\"Passage\",\"Cutoff\",\"Bridge\",\"Crossing\",\"Lane\",\"Way\",\"Run\",\"Loop\",\"Plaza\",\"Causeway\",\"Terrace\",\"Highway\",\"Bayway\",\"Circle\",\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\",\n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "city_mapping = {'St. Petersburg, FL':'St. Petersburg',\n",
    "                'St Petersburg ': 'St. Petersburg',\n",
    "                'St Pete Beach': 'St. Pete Beach',\n",
    "                'SPRING HILL': 'Spring Hill',\n",
    "                'sarasota': 'Sarasota',\n",
    "                'St Petersburg': 'St. Petersburg',\n",
    "                'lutz': 'Lutz',\n",
    "                'spring hill': 'Spring Hill',\n",
    "                'Zephyhills': 'Zephyrhills',\n",
    "                'port richey': 'Port Richey',\n",
    "                'Miakka': 'Old Myakka',\n",
    "                'Saint Petersburg': 'St. Petersburg',\n",
    "                'Seminole ': 'Seminole',\n",
    "                'Land O Lakes': \"Land O' Lakes\",\n",
    "                'Tampa ': 'Tampa',\n",
    "                'St Petersbug': 'St. Petersburg',\n",
    "                'hudson': 'Hudson',\n",
    "                'Land O Lakes, FL': \"Land O' Lakes\",\n",
    "                'Clearwarer Beach': 'Clearwater Beach',\n",
    "                'Palm Harbor, Fl.': 'Palm Harbor',\n",
    "                'tampa': 'Tampa'}\n",
    "\n",
    "cities = ['Indian Shores',\"Land O' Lakes\", 'Pinellas Park',\n",
    "          'Largo', 'Trinity', 'St. Petersburg', 'Bay Pines',\n",
    "          'Treasure Island', 'Indian Rocks Beach', 'Apollo Beach',\n",
    "          'Palm Harbor', 'Temple Terrace','Tampa', 'St. Pete Beach',\n",
    "          'Lakeland', 'Old Myakka', 'Plant City','Dunedin', 'South Highpoint', \n",
    "          'Madeira Beach', 'Gulfport', 'Lakewood Ranch', 'Longboat Key', 'Brandon',\n",
    "          'Clearwater Beach', 'Verna', 'Seminole', 'Dade City', 'Feather Sound', 'Redington Shores',\n",
    "          'Gandy', 'South Pasadena', 'Cortez Village', 'Safety Harbor', 'San Antonio',\n",
    "          'Anna Maria', 'Bradenton Beach', 'Palm Harbor',\n",
    "          'Wesley Chapel', 'Tarpon Springs', 'New Port Richey',\n",
    "          'St. Petersburg', 'Port Richey', 'Clearwater', 'Pasadena', 'Redington Beach',\n",
    "          'Holiday', 'Sarasota', 'Lutz', 'Wimauma', 'Parrish', 'Zephyrhills', 'Shady Hills',\n",
    "          'Thonotosassa', 'Belleair', 'Belleair Beach', 'Ellenton', 'Ruskin', 'Oldsmar',\n",
    "          'Valrico', 'Kenneth City', 'Hudson', 'Riverview',\n",
    "          'Bradenton', 'Odessa', 'Gibsonton', 'Lithia',\n",
    "          'Palmetto', 'Pass-a-Grille Beach', 'Spring Hill',\n",
    "          'Holmes Beach', 'Dover', 'Seffner', 'Sun City Center', 'Saint Leo']\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Av\": \"Avenue\",\n",
    "            \"AVE\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Cir\": \"Circle\",\n",
    "            \"Hwy\": \"Highway\",\n",
    "            \"Blvd.\": \"Boulevard\",\n",
    "            \"Pkwy\": \"Parkway\",\n",
    "            \"dr\": \"Drive\",\n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"Pl\": \"Place\",\n",
    "            \"Cswy\": \"Causeway\",\n",
    "            \"Plz\": \"Plaza\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Pky\": \"Parkway\",\n",
    "            \"Ln\": \"Lane\",\n",
    "            \"st\": \"Street\",\n",
    "            \"road\": \"Road\",\n",
    "            \"drive\": \"Drive\",\n",
    "            \"lane\": \"Lane\"\n",
    "            }\n",
    "\n",
    "def audit_state(state_name):\n",
    "    if state_name != 'FL':\n",
    "        state_name = 'FL'\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    try:\n",
    "        comma_index = street_name.index(',')\n",
    "        street_name = remove(street_name,comma_index)\n",
    "    except:\n",
    "        pass\n",
    "        #print street_name\n",
    "    try:\n",
    "        pound_index = street_name.index('#')\n",
    "        street_name = remove(street_name, pound_index)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        suite_index = street_name.index('Suite')\n",
    "        street_name = remove(street_name, suite_index)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    end_direction = direction_re.search(street_name)\n",
    "    if end_direction:\n",
    "        street_name = street_name[:-len(end_direction.group(0))]\n",
    "        street_name = end_direction.group(0) + \" \" + street_name\n",
    "        try:\n",
    "            street_name = update_direction(street_name,direction_mapping)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def audit_city(invalid_cities, city_name):\n",
    "    if city_name not in cities:\n",
    "        if city_name in city_mapping:\n",
    "            city_name = city_mapping[city_name]\n",
    "        else:\n",
    "            invalid_cities[city_name] += 1       \n",
    "\n",
    "def audit_zipcode(invalid_zipcodes, zipcode):\n",
    "    if not re.match(r'^\\d{5}$', zipcode):\n",
    "        invalid_zipcodes[zipcode] += 1\n",
    "             \n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def is_state(elem):\n",
    "    return (elem.attrib['k'] == \"addr:state\")\n",
    "    \n",
    "def is_city(elem):\n",
    "    return (elem.attrib['k'] == \"addr:city\")\n",
    "\n",
    "def is_zipcode(elem):\n",
    "    return 'zip' in elem.attrib['k']\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    \n",
    "    street_types = defaultdict(set)\n",
    "    city_types = defaultdict(int)\n",
    "    zipcode_types = defaultdict(int)\n",
    "    \n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "                elif is_zipcode(tag):\n",
    "                    audit_zipcode(zipcode_types, tag.attrib['v'])\n",
    "                elif is_state(tag):\n",
    "                    audit_state(tag.attrib['v'])\n",
    "                elif is_city(tag):\n",
    "                    audit_city(city_types, tag.attrib['v'])\n",
    "\n",
    "    return street_types, zipcode_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    name_array = name.split(' ')\n",
    "    last = name_array[-1]\n",
    "    name_array[-1] = mapping[last]\n",
    "    joined = ' '.join(name_array)\n",
    "    return joined\n",
    "\n",
    "def update_direction(name, mapping):\n",
    "    name_array = name.split(' ')\n",
    "    first = name_array[0]\n",
    "    name_array[0] = mapping[first]\n",
    "    return ' '.join(name_array)\n",
    "\n",
    "def remove(name,index):\n",
    "    subname = name[:index]\n",
    "    return subname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Road 52\n",
      "SR 52\n",
      "FL 52\n",
      "Boulevard of the Arts\n",
      "SR 56\n",
      "FL 56\n",
      "N U.S. Hwy 41\n",
      "US 41\n",
      "N US Highway 41\n",
      "N US 41\n",
      "Main St m104\n",
      "6010 US-301\n",
      "US-301\n",
      "State Road 64\n",
      "U.S.19\n",
      "US 98 Bypass\n",
      "Corey Ave  St Pete Beach\n",
      "4th Street Notth\n",
      "US 92\n",
      "12000 US Highway 92\n",
      "North Avenue Republica de Cuba\n",
      "Avenue Republica de Cuba\n",
      "West Brandon Blvd (S.R. 60)\n",
      "S Howard Av 105\n",
      "North Westshore Bolevard\n",
      "8492 Manatee Bay Dr Tampa\n",
      "Avenue B\n",
      "SR 580\n",
      "State Road 580\n",
      "Avenue F\n",
      "US 301\n",
      "US Highway 301\n",
      "S US Highway 301\n",
      "South US Highway 301\n",
      "us Highway 301\n",
      "CR 672\n",
      "University Square Mall\n",
      "E FL 70\n",
      "Lakewood Main St Ste 102\n",
      "FL 60\n",
      "US-19\n",
      "W Swann Av Prkg\n",
      "US 98\n",
      "US Highway 19\n",
      "US 19\n",
      "us 19\n",
      "3001 US Hwy 19\n",
      "U.S. 19\n",
      "US Hwy 19\n",
      "28519 State Road 54\n",
      "FL 54\n",
      "SR 54\n",
      "State Road 54\n",
      "Sunshine skyway\n",
      "Bay Esplanade\n",
      "US 301 (FL)\n",
      "US 19 (FL)\n",
      "S Fort Harrison\n",
      "Broadway\n",
      "defaultdict(<type 'int'>, {'33548:33556': 1, '34234:34243': 4, '33701:33704': 4, '33760;33764': 1, '33540; 33849': 2, '34683:34689': 2, '33764; 33756': 8, '33765; 33755': 18, '33569;33547': 2, '33765; 33759; 33765': 26, '33597:33809': 6, '33602:33605': 2, '33602:33603': 12, '33547:33569': 8, '34652:34655': 2, '33701:33712': 16, '34234:34236': 8, '33760; 33764': 7, '33777; 33710': 18, '33543; 34639; 33543': 7, '33777;33710': 2, '34690:34691': 2, '33647; 33543': 5, '33541:33542': 5, '33610; 33617': 2, '34209; 34205': 7, '34653:34668': 2, '33765;33759;33765': 2, '33761:34677': 6, '34233:34238': 2, '33569:33594': 8, '34653; 34652': 7, '34669; 34667; 34667': 20})\n"
     ]
    }
   ],
   "source": [
    "st_types, invalid_zips = audit(OSMFILE)\n",
    "\n",
    "for st_type, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        try:\n",
    "            better_name = update_name(name, mapping)\n",
    "            #print name, \"=>\", better_name\n",
    "        except:\n",
    "            print name\n",
    "            \n",
    "print invalid_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile data.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database.\n",
    "To do so you will parse the elements in the OSM XML file, transforming them from document format to\n",
    "tabular format, thus making it possible to write to .csv files.  These csv files can then easily be\n",
    "imported to a SQL database as tables.\n",
    "\n",
    "The process for this transformation is as follows:\n",
    "- Use iterparse to iteratively step through each top level element in the XML\n",
    "- Shape each element into several data structures using a custom function\n",
    "- Utilize a schema and validation library to ensure the transformed data is in the correct format\n",
    "- Write each data structure to the appropriate .csv files\n",
    "\n",
    "We've already provided the code needed to load the data, perform iterative parsing and write the\n",
    "output to csv files. Your task is to complete the shape_element function that will transform each\n",
    "element into the correct format. To make this process easier we've already defnameed a schema (see\n",
    "the schema.py file in the last code tab) for the .csv files and the eventual tables. Using the \n",
    "cerberus library we can validate the output against this schema to ensure it is correct.\n",
    "\n",
    "## Shape Element Function\n",
    "The function should take as input an iterparse Element object and return a dictionary.\n",
    "\n",
    "### If the element top level tag is \"node\":\n",
    "The dictionary returned should have the format {\"node\": .., \"node_tags\": ...}\n",
    "\n",
    "The \"node\" field should hold a dictionary of the following top level node attributes:\n",
    "- id\n",
    "- user\n",
    "- uid\n",
    "- version\n",
    "- lat\n",
    "- lon\n",
    "- timestamp\n",
    "- changeset\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"node_tags\" field should hold a list of dictionaries, one per secondary tag. Secondary tags are\n",
    "child tags of node which have the tag name/type: \"tag\". Each dictionary should have the following\n",
    "fields from the secondary tag attributes:\n",
    "- id: the top level node id attribute value\n",
    "- key: the full tag \"k\" attribute value if no colon is present or the characters after the colon if one is.\n",
    "- value: the tag \"v\" attribute value\n",
    "- type: either the characters before the colon in the tag \"k\" value or \"regular\" if a colon\n",
    "        is not present.\n",
    "\n",
    "Additionally,\n",
    "\n",
    "- if the tag \"k\" value contains problematic characters, the tag should be ignored\n",
    "- if the tag \"k\" value contains a \":\" the characters before the \":\" should be set as the tag type\n",
    "  and characters after the \":\" should be set as the tag key\n",
    "- if there are additional \":\" in the \"k\" value they and they should be ignored and kept as part of\n",
    "  the tag key. For example:\n",
    "\n",
    "  <tag k=\"addr:street:name\" v=\"Lincoln\"/>\n",
    "  should be turned into\n",
    "  {'id': 12345, 'key': 'street:name', 'value': 'Lincoln', 'type': 'addr'}\n",
    "\n",
    "- If a node has no secondary tags then the \"node_tags\" field should just contain an empty list.\n",
    "\n",
    "The fnameal return value for a \"node\" element should look something like:\n",
    "\n",
    "{'node': {'id': 757860928,\n",
    "          'user': 'uboot',\n",
    "          'uid': 26299,\n",
    "       'version': '2',\n",
    "          'lat': 41.9747374,\n",
    "          'lon': -87.6920102,\n",
    "          'timestamp': '2010-07-22T16:16:51Z',\n",
    "      'changeset': 5288876},\n",
    " 'node_tags': [{'id': 757860928,\n",
    "                'key': 'amenity',\n",
    "                'value': 'fast_food',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'cuisine',\n",
    "                'value': 'sausage',\n",
    "                'type': 'regular'},\n",
    "               {'id': 757860928,\n",
    "                'key': 'name',\n",
    "                'value': \"Shelly's Tasty Freeze\",\n",
    "                'type': 'regular'}]}\n",
    "\n",
    "### If the element top level tag is \"way\":\n",
    "The dictionary should have the format {\"way\": ..., \"way_tags\": ..., \"way_nodes\": ...}\n",
    "\n",
    "The \"way\" field should hold a dictionary of the following top level way attributes:\n",
    "- id\n",
    "-  user\n",
    "- uid\n",
    "- version\n",
    "- timestamp\n",
    "- changeset\n",
    "\n",
    "All other attributes can be ignored\n",
    "\n",
    "The \"way_tags\" field should again hold a list of dictionaries, following the exact same rules as\n",
    "for \"node_tags\".\n",
    "\n",
    "Additionally, the dictionary should have a field \"way_nodes\". \"way_nodes\" should hold a list of\n",
    "dictionaries, one for each nd child tag.  Each dictionary should have the fields:\n",
    "- id: the top level element (way) id\n",
    "- node_id: the ref attribute value of the nd tag\n",
    "- position: the index starting at 0 of the nd tag i.e. what order the nd tag appears within\n",
    "            the way element\n",
    "\n",
    "The fnameal return value for a \"way\" element should look something like:\n",
    "\n",
    "{'way': {'id': 209809850,\n",
    "         'user': 'chicago-buildings',\n",
    "         'uid': 674454,\n",
    "         'version': '1',\n",
    "         'timestamp': '2013-03-13T15:58:04Z',\n",
    "         'changeset': 15353317},\n",
    " 'way_nodes': [{'id': 209809850, 'node_id': 2199822281, 'position': 0},\n",
    "               {'id': 209809850, 'node_id': 2199822390, 'position': 1},\n",
    "               {'id': 209809850, 'node_id': 2199822392, 'position': 2},\n",
    "               {'id': 209809850, 'node_id': 2199822369, 'position': 3},\n",
    "               {'id': 209809850, 'node_id': 2199822370, 'position': 4},\n",
    "               {'id': 209809850, 'node_id': 2199822284, 'position': 5},\n",
    "               {'id': 209809850, 'node_id': 2199822281, 'position': 6}],\n",
    " 'way_tags': [{'id': 209809850,\n",
    "               'key': 'housenumber',\n",
    "               'type': 'addr',\n",
    "               'value': '1412'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street',\n",
    "               'type': 'addr',\n",
    "               'value': 'West Lexington St.'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:name',\n",
    "               'type': 'addr',\n",
    "               'value': 'Lexington'},\n",
    "              {'id': '209809850',\n",
    "               'key': 'street:prefix',\n",
    "               'type': 'addr',\n",
    "               'value': 'West'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'street:type',\n",
    "               'type': 'addr',\n",
    "               'value': 'Street'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'building',\n",
    "               'type': 'regular',\n",
    "               'value': 'yes'},\n",
    "              {'id': 209809850,\n",
    "               'key': 'levels',\n",
    "               'type': 'building',\n",
    "               'value': '1'},\n",
    "              {'id': 209809850,\n",
    "\n",
    "               'key': 'building_id',\n",
    "               'type': 'chicago',\n",
    "               'value': '366409'}]}\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"tampa_florida.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in NODE_FIELDS:\n",
    "                node_attribs[attrib] = element.attrib[attrib]\n",
    "        for child in element:\n",
    "            node_tag = {}\n",
    "            if LOWER_COLON.match(child.attrib['k']):\n",
    "                node_tag['type'] = child.attrib['k'].split(':',1)[0]\n",
    "                node_tag['key'] = child.attrib['k'].split(':',1)[1]\n",
    "                node_tag['id'] = element.attrib['id']\n",
    "                node_tag['value'] = child.attrib['v']\n",
    "                tags.append(node_tag)\n",
    "            elif PROBLEMCHARS.match(child.attrib['k']):\n",
    "                continue\n",
    "            else:\n",
    "                node_tag['type'] = 'regular'\n",
    "                node_tag['key'] = child.attrib['k']\n",
    "                node_tag['id'] = element.attrib['id']\n",
    "                node_tag['value'] = child.attrib['v']\n",
    "                tags.append(node_tag)\n",
    "        \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "        \n",
    "    elif element.tag == 'way':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in WAY_FIELDS:\n",
    "                way_attribs[attrib] = element.attrib[attrib]\n",
    "        \n",
    "        position = 0\n",
    "        for child in element:\n",
    "            way_tag = {}\n",
    "            way_node = {}\n",
    "            \n",
    "            if child.tag == 'tag':\n",
    "                if LOWER_COLON.match(child.attrib['k']):\n",
    "                    way_tag['type'] = child.attrib['k'].split(':',1)[0]\n",
    "                    way_tag['key'] = child.attrib['k'].split(':',1)[1]\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    way_tag['value'] = child.attrib['v']\n",
    "                    tags.append(way_tag)\n",
    "                elif PROBLEMCHARS.match(child.attrib['k']):\n",
    "                    continue\n",
    "                else:\n",
    "                    way_tag['type'] = 'regular'\n",
    "                    way_tag['key'] = child.attrib['k']\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    way_tag['value'] = child.attrib['v']\n",
    "                    tags.append(way_tag)\n",
    "                    \n",
    "            elif child.tag == 'nd':\n",
    "                way_node['id'] = element.attrib['id']\n",
    "                way_node['node_id'] = child.attrib['ref']\n",
    "                way_node['position'] = position\n",
    "                position += 1\n",
    "                way_nodes.append(way_node)\n",
    "        \n",
    "    return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_map('tampa_florida.osm',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%writefile database.py\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('tampa.db')\n",
    "conn.text_factory = str\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('CREATE TABLE nodes (id INTEGER PRIMARY KEY NOT NULL, lat REAL, lon REAL, user TEXT, uid INTEGER, version INTEGER, changeset INTEGER, timestamp TEXT);')\n",
    "with open('nodes.csv','rb') as fname:\n",
    "    dr = csv.DictReader(fname)\n",
    "    to_db = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) \\\n",
    "             for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes (id, lat, lon, user, uid, version, changeset, timestamp) \\\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE nodes_tags (id INTEGER, key TEXT, value TEXT, type TEXT, FOREIGN KEY (id) REFERENCES nodes(id));\")\n",
    "with open('nodes_tags.csv','rb') as fname:\n",
    "    dr = csv.DictReader(fname) \n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes_tags (id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways (id INTEGER PRIMARY KEY NOT NULL, user TEXT, uid INTEGER, version TEXT, changeset INTEGER, timestamp TEXT);\")\n",
    "with open('ways.csv','rb') as fname:\n",
    "    dr = csv.DictReader(fname) \n",
    "    to_db = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways (id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways_nodes (id INTEGER NOT NULL, node_id INTEGER NOT NULL, position INTEGER NOT NULL, FOREIGN KEY (id) REFERENCES ways(id), FOREIGN KEY (node_id) REFERENCES nodes(id));\")\n",
    "with open('ways_nodes.csv','rb') as fname:\n",
    "    dr = csv.DictReader(fname) \n",
    "    to_db = [(i['id'], i['node_id'], i['position']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_nodes (id, node_id, position) VALUES (?, ?, ?);\", to_db)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(\"CREATE TABLE ways_tags (id INTEGER NOT NULL, key TEXT NOT NULL, value TEXT NOT NULL, type TEXT, FOREIGN KEY (id) REFERENCES ways(id));\")\n",
    "with open('ways_tags.csv','rb') as fname:\n",
    "    dr = csv.DictReader(fname) \n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags (id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:\n",
      "1655566\n",
      "Number of ways:\n",
      "182866\n",
      "Number of unique users:\n",
      "1448\n",
      "Top 10 contributing users:\n",
      "[('coleman', 258302), ('woodpeck_fixbot', 235013), ('grouper', 187215), ('EdHillsman', 106677), ('NE2', 72924), ('David Hey', 60918), ('LnxNoob', 58364), ('KalininOV', 48825), ('westampa', 42145), ('bot-mode', 37656)]\n",
      "Number of users contributing once:\n",
      "Top 10 amenities:\n",
      "[('restaurant', 852), ('place_of_worship', 771), ('school', 553), ('fast_food', 396), ('bicycle_parking', 353), ('bench', 279), ('fuel', 235), ('fountain', 201), ('bank', 170), ('toilets', 148)]\n",
      "Top 5 places of worship:\n",
      "[('christian', 724), ('jewish', 4), ('bahai', 3), ('buddhist', 3), ('unitarian_universalist', 3)]\n",
      "Top 5 cuisines\n",
      "[('american', 93), ('pizza', 70), ('mexican', 41), ('italian', 28), ('seafood', 25)]\n",
      "Top 10 restaurants:\n",
      "[('Tijuana Flats', 8), (\"Applebee's\", 6), ('Bob Evans', 6), (\"Denny's\", 6), ('IHOP', 6), ('Outback Steakhouse', 6), ('Panera Bread', 6), (\"Chili's\", 5), ('Golden Corral', 5), ('Pizza Hut', 5)]\n"
     ]
    }
   ],
   "source": [
    "#%%writefile query.py\n",
    "\n",
    "print \"Number of nodes:\"\n",
    "print cur.execute('SELECT COUNT(*) FROM nodes').fetchone()[0]\n",
    "\n",
    "print \"Number of ways:\"\n",
    "print cur.execute('SELECT COUNT(*) FROM ways').fetchone()[0]\n",
    "\n",
    "print \"Number of unique users:\"\n",
    "print cur.execute('SELECT COUNT(DISTINCT(e.uid)) \\\n",
    "            FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e').fetchone()[0]\n",
    "\n",
    "print \"Top 10 contributing users:\"\n",
    "users = []\n",
    "for row in cur.execute('SELECT e.user, COUNT(*) as num \\\n",
    "            FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "            GROUP BY e.user \\\n",
    "            ORDER BY num DESC \\\n",
    "            LIMIT 10'):\n",
    "    users.append(row)\n",
    "print users\n",
    "\n",
    "print \"Number of users contributing once:\"\n",
    "cur.execute('SELECT COUNT(*) FROM \\\n",
    "                (SELECT e.user, COUNT(*) as num \\\n",
    "                 FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "                 GROUP BY e.user \\\n",
    "                 HAVING num=1) u').fetchone()[0]\n",
    "\n",
    "print \"Top 10 amenities:\"\n",
    "amenities = []\n",
    "for row in cur.execute('SELECT value, COUNT(*) as num \\\n",
    "            FROM nodes_tags \\\n",
    "            WHERE key=\"amenity\" \\\n",
    "            GROUP BY value \\\n",
    "            ORDER BY num DESC \\\n",
    "            LIMIT 10'):\n",
    "    amenities.append(row)\n",
    "print amenities\n",
    "\n",
    "print \"Top 5 places of worship:\"\n",
    "religions = []\n",
    "for row in cur.execute('SELECT nodes_tags.value, COUNT(*) as num \\\n",
    "            FROM nodes_tags \\\n",
    "                JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value=\"place_of_worship\") i \\\n",
    "                ON nodes_tags.id=i.id \\\n",
    "            WHERE nodes_tags.key=\"religion\" \\\n",
    "            GROUP BY nodes_tags.value \\\n",
    "            ORDER BY num DESC \\\n",
    "            LIMIT 5;'):\n",
    "    religions.append(row)\n",
    "print religions\n",
    "\n",
    "print \"Top 5 cuisines\"\n",
    "cuisines = []\n",
    "for row in cur.execute('SELECT nodes_tags.value, COUNT(*) as num \\\n",
    "            FROM nodes_tags \\\n",
    "                JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value=\"restaurant\") i \\\n",
    "                ON nodes_tags.id=i.id \\\n",
    "            WHERE nodes_tags.key=\"cuisine\" \\\n",
    "            GROUP BY nodes_tags.value \\\n",
    "            ORDER BY num DESC \\\n",
    "            LIMIT 5'):\n",
    "    cuisines.append(row)\n",
    "print cuisines\n",
    "\n",
    "print \"Top 10 restaurants:\"\n",
    "restaurants = []\n",
    "for row in cur.execute('SELECT value, COUNT(*) as num \\\n",
    "            FROM nodes_tags \\\n",
    "                JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value=\"restaurant\") i \\\n",
    "                ON nodes_tags.id=i.id \\\n",
    "            WHERE key=\"name\"\\\n",
    "            GROUP BY value \\\n",
    "            ORDER BY num DESC \\\n",
    "            LIMIT 10'):\n",
    "    restaurants.append(row)\n",
    "print restaurants"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
